{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4z8PU4QjYDS"
   },
   "source": [
    "# Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyE_c8nhuS3n"
   },
   "source": [
    "## Caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx0bx2xZ4IqN",
    "outputId": "63a31f5f-e088-4009-bfef-dbf4a9587400"
   },
   "outputs": [],
   "source": [
    "datasets_path     = '/homeLocal/praticas-cv-cnn/datasets/'\n",
    "models_path       = '/homeLocal/praticas-cv-cnn/models/'\n",
    "tensorboard_path  = '/homeLocal/praticas-cv-cnn/Tensorboard/lenet5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGVK_48hDFb"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t63l-AeYhE02"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def my_imshow(img, dataset, numImages=10):\n",
    "  \n",
    "    if dataset == 'cifar10' : \n",
    "        img = img / 2 + 0.5     # unnormalize\n",
    "    \n",
    "    img = torchvision.utils.make_grid(img[:numImages],nrow=numImages//2)\n",
    "    \n",
    "    npimg = img.numpy()    \n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()\n",
    "\n",
    "def show_images(train_loader, test_loader, dataset, numImages=10) :\n",
    "    print('Train samples')\n",
    "    # get some random training images\n",
    "    dataiter = iter(train_loader)\n",
    "    images = next(dataiter)[0]\n",
    "    my_imshow(images, dataset, numImages)\n",
    "\n",
    "    print('Test samples')\n",
    "    # get some random training images\n",
    "    dataiter = iter(test_loader)\n",
    "    images = next(dataiter)[0]\n",
    "    my_imshow(images, dataset, numImages)\n",
    "\n",
    "def get_data_cifar10 ( batch_size , show_image=False, numImages=10 ) :\n",
    "  \n",
    "    my_transform = torchvision.transforms.Compose([\n",
    "                            torchvision.transforms.Resize(28),\n",
    "                            torchvision.transforms.ToTensor(),\n",
    "                            torchvision.transforms.Normalize(mean=[0.5],std=[0.5])\n",
    "                                    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "                                root=f'{datasets_path}/train/', \n",
    "                                train=True, \n",
    "                                transform=my_transform, \n",
    "                                download=False\n",
    "                                )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "                                root=f'{datasets_path}/test/',\n",
    "                                train=False, \n",
    "                                transform=my_transform, \n",
    "                                download=False\n",
    "                                )\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True\n",
    "                                )\n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False\n",
    "                            )\n",
    "    \n",
    "    if show_image :\n",
    "        show_images(train_loader, test_loader, 'cifar10', numImages)\n",
    "    \n",
    "    return train_loader, test_loader, len(train_dataset)\n",
    "\n",
    "\n",
    "def get_data_mnist ( batch_size , show_image=False, numImages=10 ) :\n",
    "  \n",
    "    train_dataset = torchvision.datasets.mnist.MNIST(\n",
    "                            root=f'{datasets_path}/train/', \n",
    "                            train=True, \n",
    "                            transform=torchvision.transforms.ToTensor(), \n",
    "                            download=False\n",
    "                            )\n",
    "    test_dataset = torchvision.datasets.mnist.MNIST(\n",
    "                            root=f'{datasets_path}/test/',\n",
    "                            train=False, \n",
    "                            transform=torchvision.transforms.ToTensor(), \n",
    "                            download=False\n",
    "                            )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    if show_image :\n",
    "        show_images(train_loader, test_loader, 'mnist', numImages)\n",
    "    \n",
    "    return train_loader, test_loader, len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "w1xTo7PfW9T3",
    "outputId": "d855abbc-e96d-4a91-f670-8fe63ce303cd"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_data_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_image\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumImages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mget_data_mnist\u001b[39m\u001b[34m(batch_size, show_image, numImages)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data_mnist\u001b[39m ( batch_size , show_image=\u001b[38;5;28;01mFalse\u001b[39;00m, numImages=\u001b[32m10\u001b[39m ) :\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     train_dataset = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmnist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatasets_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/train/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     76\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     test_dataset = torchvision.datasets.mnist.MNIST(\n\u001b[32m     78\u001b[39m                             root=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatasets_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/test/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     79\u001b[39m                             train=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m     80\u001b[39m                             transform=torchvision.transforms.ToTensor(), \n\u001b[32m     81\u001b[39m                             download=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     82\u001b[39m                             )\n\u001b[32m     84\u001b[39m     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/src/venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:103\u001b[39m, in \u001b[36mMNIST.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.download()\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_exists():\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.data, \u001b[38;5;28mself\u001b[39m.targets = \u001b[38;5;28mself\u001b[39m._load_data()\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "get_data_mnist(batch_size=256, show_image=True, numImages=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T07l5G-CjffZ"
   },
   "outputs": [],
   "source": [
    "get_data_cifar10(batch_size=256, show_image=True, numImages=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUZVS7myuD7Z"
   },
   "source": [
    "# Rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QORACe2Dgk_H"
   },
   "source": [
    "## Arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8yJxo8_LKQg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module) :\n",
    "  \n",
    "    def __init__(self, num_classes=10, n_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, debug=False):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjE_rDZOmfd6"
   },
   "source": [
    "## Informações sobre a rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdSgF-6wD7__",
    "outputId": "a5085295-1a0d-45c5-f982-fe0aa0cf5703"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Running on {my_device.type}.\")\n",
    "\n",
    "net = LeNet(num_classes=10, n_channels=1)\n",
    "net = net.to(my_device)\n",
    "\n",
    "a = torch.rand( (1, 3, 28, 28) )\n",
    "\n",
    "b = net( a.to(my_device) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_TfzVGVRrsL",
    "outputId": "fcd47a54-96fc-4947-f790-e05c8c0c98e8"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, input_size=(3,28,28), batch_size=256)\n",
    "del net, a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b13rYc5AuT1J"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMTNH0AbhSfo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.optim \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "  \n",
    "from datetime import datetime\n",
    "\n",
    "def train ( dataset, epochs=100, lr=1e-1, prefix='', upper_bound=99.0, device='cpu',\n",
    "           save=False, debug=False, plot_histograms=False, lambda_reg=0) :\n",
    "\n",
    "    if dataset == 'mnist' :\n",
    "        batch_size = 256\n",
    "        train_loader, test_loader, dataset_size = get_data_mnist(batch_size, \n",
    "                                                                    show_image=True\n",
    "                                                                    )\n",
    "        num_classes = 10\n",
    "        n_channels = 1\n",
    "    elif dataset == 'cifar10' :\n",
    "        batch_size = 256\n",
    "        train_loader, test_loader, dataset_size = get_data_cifar10(batch_size, \n",
    "                                                                    show_image=True\n",
    "                                                                    )\n",
    "        num_classes = 10\n",
    "        n_channels = 3\n",
    "    else :\n",
    "        print('Dataset loader not implemented.')\n",
    "        return None    \n",
    "  \n",
    "    net = LeNet( num_classes, n_channels )\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr, weight_decay=lambda_reg)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    now = datetime.now()\n",
    "    suffix = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    prefix = prefix + '-' + suffix if prefix != '' else suffix\n",
    "\n",
    "    writer = SummaryWriter( log_dir=tensorboard_path+prefix )\n",
    "    \n",
    "    writer.add_graph(net, next(iter(train_loader))[0].to(my_device))\n",
    "\n",
    "    accuracies = []\n",
    "    max_accuracy = -1.0\n",
    "\n",
    "    for epoch in tqdm( range(epochs) , desc='Training epochs...' ) :\n",
    "        net.train()  \n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            train_x = train_x.to(device)\n",
    "            train_label = train_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predict_y = net( train_x )[0]\n",
    "\n",
    "            # Loss:\n",
    "            error = loss( predict_y , train_label.long() )\n",
    "\n",
    "            writer.add_scalar( 'Loss/train', error, \n",
    "                            idx+( epoch*(dataset_size//batch_size) ) )\n",
    "\n",
    "            error.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy:\n",
    "            predict_ys = torch.max( predict_y, axis=1 )[1]\n",
    "            correct    = torch.sum(predict_ys == train_label)\n",
    "\n",
    "            writer.add_scalar( 'Accuracy/train', correct/train_x.size(0), \n",
    "                            idx+( epoch*(dataset_size//batch_size) ) )\n",
    "\n",
    "            if debug and idx % 10 == 0 :\n",
    "                print(f'idx: {idx}, _error: {error}')\n",
    "\n",
    "        if plot_histograms : \n",
    "            plot_histograms_tensorboard(writer, net, epoch)\n",
    "        \n",
    "        accuracy = validate(net, test_loader, device=device)\n",
    "        accuracies.append(accuracy.cpu())\n",
    "        writer.add_scalar( 'Accuracy/test', accuracy, epoch )\n",
    "    \n",
    "        if accuracy > max_accuracy:\n",
    "            best_model = copy.deepcopy(net)\n",
    "            max_accuracy = accuracy\n",
    "            print(f'Saving Best Model with Accuracy: {max_accuracy:3.4f}')\n",
    "            \n",
    "        print( f'Epoch: {epoch+1:3d} | Accuracy : {accuracy:3.4f}%' )\n",
    "\n",
    "        if accuracy > upper_bound :\n",
    "            break\n",
    "   \n",
    "    if save : \n",
    "        path = f'{models_path}LeNet5-{dataset}-{max_accuracy:.2f}.pkl'\n",
    "        torch.save(best_model.state_dict(), path)\n",
    "        print('Model saved in:',path)\n",
    "  \n",
    "    plt.plot(accuracies)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms_tensorboard ( writer, net, epoch ) :\n",
    "    writer.add_histogram('Bias/conv1',   net.c1.bias,        epoch)\n",
    "    writer.add_histogram('Weight/conv1', net.c1.weight,      epoch)\n",
    "    writer.add_histogram('Grad/conv1',   net.c1.weight.grad, epoch)\n",
    "\n",
    "    writer.add_histogram('Bias/conv3',   net.c3.bias,        epoch)\n",
    "    writer.add_histogram('Weight/conv3', net.c3.weight,      epoch)\n",
    "    writer.add_histogram('Grad/conv3',   net.c3.weight.grad, epoch)\n",
    "\n",
    "    writer.add_histogram('Bias/conv5',   net.c5.bias,        epoch)\n",
    "    writer.add_histogram('Weight/conv5', net.c5.weight,      epoch)\n",
    "    writer.add_histogram('Grad/conv5',   net.c5.weight.grad, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXG-IjwogweA"
   },
   "source": [
    "## Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFNT61DWgzW2"
   },
   "outputs": [],
   "source": [
    "def validate ( model , data , device='cpu') :\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    sum = 0\n",
    "    \n",
    "    for idx, (test_x, test_label) in enumerate(data) : \n",
    "        test_x = test_x.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "        predict_y = model( test_x )[0].detach()\n",
    "        predict_ys = torch.max( predict_y, axis=1 )[1]\n",
    "        sum = sum + test_x.size(0)\n",
    "        correct = correct + torch.sum(predict_ys == test_label)\n",
    "    \n",
    "    return correct*100./sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuR0UzNEMCy1"
   },
   "source": [
    "# Execução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez-wfLE-ET_3"
   },
   "source": [
    "## Treina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "u_naT_XMdkys",
    "outputId": "a5c02cc0-2017-4783-a9a6-f7ea0d93f244"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "print(f\"Running on {my_device.type}.\")\n",
    "\n",
    "epochs = 10\n",
    "dataset = 'mnist' # 'cifar10' \n",
    "lr = 1.3e0\n",
    "lambda_reg = 0\n",
    "\n",
    "prefix = 'LeNet-{}-e-{}-lr-{}'.format(dataset, epochs, lr)\n",
    "\n",
    "net = train( dataset=dataset, epochs=epochs, lr=lr, prefix=prefix , upper_bound=100, device=my_device,\n",
    "            save=True, debug=False, plot_histograms=True, lambda_reg=lambda_reg )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36XRAIEHubdR"
   },
   "source": [
    "# Carregar Rede de arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOssniG-mWSN"
   },
   "outputs": [],
   "source": [
    "del net\n",
    "\n",
    "path = '/homeLocal/praticas-cv-cnn/models/LeNet5-mnist-95.52.pkl'\n",
    "n_channels = 1\n",
    "\n",
    "def load_LeNet ( device , path ) :\n",
    "    net = LeNet(num_classes=10, n_channels=n_channels)\n",
    "    net = net.to(device)\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "net = load_LeNet(my_device, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kynNDloueji"
   },
   "source": [
    "# Carregar dado do MNIST e inferir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "7kbmb6Hjf394",
    "outputId": "93599d6d-6dc8-4a87-f770-235eacb04780"
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "def sample_and_predict ( seed=None ) :\n",
    "\n",
    "    if seed is not None :\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "                                      root=f'{datasets_path}/test/', \n",
    "                                      train=False\n",
    "                                      )\n",
    " \n",
    "    i = np.random.randint(len(dataset))\n",
    "    sample = dataset[i][0]\n",
    "\n",
    "    x = torchvision.transforms.ToTensor()(sample).float()\n",
    "\n",
    "    x = x.unsqueeze_(0)\n",
    "\n",
    "    x = x.to(my_device)\n",
    "\n",
    "    y = net ( x )[1]\n",
    "    confidence = torch.max(y, 1)[0]\n",
    "    prediction = torch.max(y, 1)[1]\n",
    "\n",
    "    print( 'Sample: {}'.format(i) )\n",
    "    plt.axis('off')\n",
    "    plt.imshow( sample , cmap='gray')\n",
    "\n",
    "    confidence = confidence.data.cpu().numpy()[0]\n",
    "    prediction = prediction.data.cpu().numpy()[0]\n",
    "\n",
    "    return prediction, confidence, dataset[i][1]\n",
    "\n",
    "prediction, confidence, label = sample_and_predict()\n",
    "print( f'\\nPredicted clas: {prediction} \\nClassifier confidence: {confidence*100:4.2f}% \\nTrue label: {label}' )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rUZVS7myuD7Z",
    "QjE_rDZOmfd6",
    "36XRAIEHubdR",
    "Cf2RJcWTuigS"
   ],
   "name": "pytorch-LeNet-5-YannLecun-Paper.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
